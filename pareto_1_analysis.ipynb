{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e0e8205-29bf-4c66-bc22-a1731bb28834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARETO FRONT STUDIES NOTEBOOK #1 ===\n",
    "# Author: Ana Sofia Calle Mu√±oz\n",
    "\n",
    "# This notebook allows you to perform a Pareto analysis for a specific scheduler over a given number of trials and epochs.\n",
    "# Losses used: NLL and a sum of standard deviations sigma regularizer.\n",
    "# !!! Throughout the notebooks, look for comments including \"==\" at the beginning, as they need to be modified on your end.\n",
    "\n",
    "# == Before running:\n",
    "# Download the 4 notebooks and schedulers.py file & upload to your cluster.\n",
    "# Make sure you have the latest OptimizedDataGenerator_v2 python file.\n",
    "# Upload the model you want to work with. This nb is set to work with the Conv2D Max model & 16x16 sensor size.\n",
    "# Modify the train & validation TFrecords folder paths. This nb works with dataset_3sr filtered with labels.\n",
    "# Modify the \"intermediate_dir\" path, that is where your results will save.\n",
    "# Modify the sample hyperparameter ranges as you like. They are in the run_trials funtion.\n",
    "\n",
    "# == Run:\n",
    "# Go to the last block and look for the run_trials call. Set a scheduler, experiment name, number of trials and epochs. That's it!\n",
    "\n",
    "# Any questions you have, you can reach out on the FastML slack as Ana Sofia Calle, or callea@purdue.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc98d975-f552-42b0-bf70-8b9c62ececc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 01:06:08.543890: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-24 01:06:08.543952: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-24 01:06:08.545535: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-24 01:06:08.553650: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-24 01:06:09.545082: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping, Callback, LambdaCallback\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.metrics import Mean\n",
    "\n",
    "import os\n",
    "import random\n",
    "import csv\n",
    "\n",
    "from OptimizedDataGenerator_v2 import OptimizedDataGenerator\n",
    "from schedulers import *\n",
    "import pickle\n",
    "from models_16x16.models import *\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f177bc1-468a-4add-bd84-1151e0ebdd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59055ee-2084-43ce-bf3b-a298cf903d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==TFrecords paths\n",
    "# I recommend using contained datasets for better results\n",
    "\n",
    "tfrecords_dir_train = \"/home/callea/TFrecords_3src_filtered/train\"\n",
    "tfrecords_dir_validation = \"/home/callea/TFrecords_3src_filtered/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e7cb5-99e1-4851-9b2f-abc49d9dcc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss: NLL and a sum of standard deviations sigma regularizer (you can modify the regularizer).\n",
    "\n",
    "current_reg_weight = tf.Variable(0.0, trainable=False, dtype=tf.float32, name='reg_weight')\n",
    "\n",
    "def custom_loss(y, p_base, minval=1e-9, maxval=1e9, scale = 512):\n",
    "\n",
    "    reg_weight = current_reg_weight\n",
    "    \n",
    "    p = p_base\n",
    "    \n",
    "    mu = p[:, 0:8:2]\n",
    "    \n",
    "    # creating each matrix element in 4x4\n",
    "    Mdia = minval + tf.math.maximum(p[:, 1:8:2], 0.0)\n",
    "    Mcov = p[:,8:]\n",
    "    \n",
    "    # placeholder zero element\n",
    "    zeros = tf.zeros_like(Mdia[:,0])\n",
    "    \n",
    "    # assembles scale_tril matrix\n",
    "    row1 = tf.stack([Mdia[:,0],zeros,zeros,zeros])\n",
    "    row2 = tf.stack([Mcov[:,0],Mdia[:,1],zeros,zeros])\n",
    "    row3 = tf.stack([Mcov[:,1],Mcov[:,2],Mdia[:,2],zeros])\n",
    "    row4 = tf.stack([Mcov[:,3],Mcov[:,4],Mcov[:,5],Mdia[:,3]])\n",
    "\n",
    "    scale_tril = tf.transpose(tf.stack([row1,row2,row3,row4]),perm=[2,0,1])\n",
    "\n",
    "    dist = tfp.distributions.MultivariateNormalTriL(loc = mu, scale_tril = scale_tril) \n",
    "    \n",
    "    likelihood = dist.prob(y)  \n",
    "    likelihood = tf.clip_by_value(likelihood,minval,maxval)\n",
    "\n",
    "    NLL = -1*tf.math.log(likelihood)\n",
    "\n",
    "    cov_matrix = tf.matmul(scale_tril, tf.transpose(scale_tril, [0, 2, 1])) \n",
    "    variances = tf.linalg.diag_part(cov_matrix)\n",
    "    stds = tf.sqrt(variances + minval)\n",
    "\n",
    "    sigma_regularizer_1 = tf.reduce_sum(stds, axis=1)\n",
    "\n",
    "    batch_size = tf.shape(y)[0]\n",
    "    \n",
    "    track_loss_values(NLL, sigma_regularizer_1)\n",
    "\n",
    "    total_loss = NLL + (sigma_regularizer_1 * reg_weight)\n",
    "    \n",
    "    return tf.keras.backend.sum(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70926e82-659c-4ee4-bcc1-6b6912cb78a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scheduler that scales and saves the nll and reg mean loss\n",
    "class EpochValidationSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_data, reg_weight):\n",
    "        super().__init__()\n",
    "        self.val_data = val_data\n",
    "        self.reg_weight = reg_weight\n",
    "        self.intermediate_points = []\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        reset_loss_trackers()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for x_val, y_val in self.val_data:\n",
    "            y_pred = self.model(x_val, training=False)\n",
    "\n",
    "            mu = y_pred[:, 0:8:2]\n",
    "            Mdia = 1e-9 + tf.math.maximum(y_pred[:, 1:8:2], 0.0)\n",
    "            Mcov = y_pred[:, 8:]\n",
    "\n",
    "            zeros = tf.zeros_like(Mdia[:, 0])\n",
    "            row1 = tf.stack([Mdia[:, 0], zeros, zeros, zeros], axis=1)\n",
    "            row2 = tf.stack([Mcov[:, 0], Mdia[:, 1], zeros, zeros], axis=1)\n",
    "            row3 = tf.stack([Mcov[:, 1], Mcov[:, 2], Mdia[:, 2], zeros], axis=1)\n",
    "            row4 = tf.stack([Mcov[:, 3], Mcov[:, 4], Mcov[:, 5], Mdia[:, 3]], axis=1)\n",
    "            scale_tril = tf.stack([row1, row2, row3, row4], axis=1)\n",
    "\n",
    "            dist = tfp.distributions.MultivariateNormalTriL(loc=mu, scale_tril=scale_tril)\n",
    "            likelihood = tf.clip_by_value(dist.prob(y_val), 1e-9, 1e9)\n",
    "\n",
    "            NLL = -tf.math.log(likelihood)\n",
    "            cov_matrix = tf.matmul(scale_tril, tf.transpose(scale_tril, [0, 2, 1]))\n",
    "            stds = tf.sqrt(tf.linalg.diag_part(cov_matrix) + 1e-9)\n",
    "            sigma_regularizer_1 = tf.reduce_sum(stds, axis=1)\n",
    "\n",
    "            track_loss_values(NLL, sigma_regularizer_1)\n",
    "\n",
    "        # Calculates total samples in val_data\n",
    "        num_val_samples = sum(x.shape[0] for x, _ in self.val_data)\n",
    "\n",
    "        # Obtain mean metrics per sample (accumulates values batch by batch and computes the average per sample)\n",
    "        # This approach is not affected by batch size/shuffling of the dataset.\n",
    "        metrics = get_loss_metrics()\n",
    "        nll_mean = metrics['nll_mean']\n",
    "        reg_mean = metrics['reg_mean']\n",
    "\n",
    "        nll_total = nll_mean * num_val_samples\n",
    "        reg_total = reg_mean * num_val_samples\n",
    "        reg_weight_value = float(self.reg_weight.numpy())\n",
    "        total_loss = nll_total + reg_weight_value * reg_total\n",
    "\n",
    "        self.keras_style_val_loss = logs.get(\"val_loss\")\n",
    "        # scale the losses\n",
    "        logs['val_loss'] = total_loss / num_val_samples\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] NLL_mean={nll_mean:.6f}, REG_mean={reg_mean:.6f}\")\n",
    "        self.intermediate_points.append((nll_mean, reg_mean))\n",
    "\n",
    "def get_epoch_callback(validation_generator, reg_weight):\n",
    "    saver = EpochValidationSaver(val_data=validation_generator, reg_weight=reg_weight)\n",
    "    return saver, saver\n",
    "\n",
    "# Scheduler that earlystops trials with non-improving loss\n",
    "class EarlyStopNoImprovement(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, patience=2, min_delta=0.1, threshold=19.0, min_val_loss_to_keep=10.0):\n",
    "        super().__init__()\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.threshold = threshold\n",
    "        self.min_val_loss_to_keep = min_val_loss_to_keep\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait = 0\n",
    "        self.early_stop_triggered = False\n",
    "        self.bad_trial_due_to_high_loss = False \n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_loss = logs.get(\"val_loss\")\n",
    "        if val_loss is None:\n",
    "            return\n",
    "\n",
    "        if val_loss <= self.threshold:\n",
    "            print(f\"val_loss={val_loss:.4f} is below the threshold ({self.threshold}), early stopping is not applied.\")\n",
    "            return\n",
    "\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            print(f\"Improvement detected: val_loss decreased from {self.best_loss:.4f} to {val_loss:.4f}\")\n",
    "            self.best_loss = val_loss\n",
    "            self.wait = 0\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            print(f\"No significant improvement for {self.wait} epochs (val_loss={val_loss:.4f})\")\n",
    "\n",
    "        if self.wait >= self.patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to stagnation (val_loss={val_loss:.4f})\")\n",
    "            self.early_stop_triggered = True\n",
    "            if self.best_loss > self.min_val_loss_to_keep:\n",
    "                self.bad_trial_due_to_high_loss = True\n",
    "            self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205f0552-a9e8-47ec-8bdf-4f7f6a29d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to choose the scheduler you want to study\n",
    "# You can add more in the schedulers.py file and add an extra elif here\n",
    "\n",
    "def get_scheduler(scheduler_type, reg_weight_var, **kwargs):\n",
    "    if scheduler_type == \"cosine\":\n",
    "        return CosineScheduler(**kwargs, reg_weight_var=reg_weight_var)\n",
    "    elif scheduler_type == \"linear\":\n",
    "        return LinearScheduler(**kwargs, reg_weight_var=reg_weight_var)\n",
    "    elif scheduler_type == \"adaptive\":\n",
    "        return AdaptiveScheduler(**kwargs, reg_weight_var=reg_weight_var)\n",
    "    elif scheduler_type == \"sigmoid\":\n",
    "        return SigmoidScheduler(**kwargs, reg_weight_var=reg_weight_var)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler type: {scheduler_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed43415-fdc9-4bdf-b906-cd6250e9aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==Folder path where your results will save!!\n",
    "intermediate_dir = \"/home/callea/smart-pixels-ml/intermediate_logs\"\n",
    "os.makedirs(intermediate_dir, exist_ok=True)\n",
    "\n",
    "def objective_manual(trial_id, lambda_init, lambda_final, stop_threshold, experiment_name, scheduler_type, scheduler_kwargs, epochs):\n",
    "    global intermediate_dir\n",
    "    reset_loss_trackers()\n",
    "    current_reg_weight.assign(scheduler_kwargs['start'])\n",
    "\n",
    "    # Create and compile model\n",
    "    input_shape = (16, 16, 2)\n",
    "    model = CreateModel(input_shape, n_filters=5, pool_size=3)\n",
    "    model.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=1e-3), loss=custom_loss)\n",
    "\n",
    "    # Data generators\n",
    "    training_generator = OptimizedDataGenerator(\n",
    "        load_from_tfrecords_dir=tfrecords_dir_train,\n",
    "        shuffle=True, seed=13, quantize=True\n",
    "    )\n",
    "    validation_generator = OptimizedDataGenerator(\n",
    "        load_from_tfrecords_dir=tfrecords_dir_validation,\n",
    "        shuffle=False, seed=13, quantize=True\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    val_callback, saver = get_epoch_callback(validation_generator, reg_weight=current_reg_weight)\n",
    "    \n",
    "    if scheduler_type == \"adaptive\":\n",
    "        valid_scheduler_keys = {\"start\", \"step\", \"patience\"}\n",
    "    else:\n",
    "        valid_scheduler_keys = {\"start\", \"end\", \"max_epochs\", \"step\", \"patience\", \"sharpness\"}\n",
    "    \n",
    "    filtered_kwargs = {k: v for k, v in scheduler_kwargs.items() if k in valid_scheduler_keys}\n",
    "    \n",
    "    scheduler = get_scheduler(\n",
    "        scheduler_type=scheduler_type,\n",
    "        reg_weight_var=current_reg_weight,\n",
    "        **filtered_kwargs\n",
    "    )\n",
    "\n",
    "    bad_loss = EarlyStopNoImprovement(patience=2, min_delta=0.1)\n",
    "\n",
    "    # Training\n",
    "    model.fit(\n",
    "        training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=scheduler_kwargs.get(\"max_epochs\", epochs),\n",
    "        callbacks=[scheduler, val_callback, bad_loss],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Intermediate points file path\n",
    "    # Thoughout the training of a trial, every nll x reg validation loss point will save. These are called intermediate points.\n",
    "    exp_dir = os.path.join(intermediate_dir, experiment_name)\n",
    "    os.makedirs(exp_dir, exist_ok=True)\n",
    "    inter_path = os.path.join(exp_dir, f\"trial_{trial_id}_intermediate.pkl\")\n",
    "    \n",
    "    keras_val_loss = val_callback.keras_style_val_loss\n",
    "\n",
    "    # Trial info structure\n",
    "    trial_info = {\n",
    "        \"trial_id\": trial_id,\n",
    "        \"scheduler\": scheduler_type,\n",
    "        **scheduler_kwargs,\n",
    "        \"valid_trial\": False,\n",
    "        \"final_nll\": None,\n",
    "        \"final_reg\": None,\n",
    "        \"final_val_loss\": None,\n",
    "        \"keras_val_loss\": keras_val_loss\n",
    "    }\n",
    "\n",
    "    if getattr(bad_loss, \"bad_trial_due_to_high_loss\", False):\n",
    "        print(f\"üóëÔ∏è Trial {trial_id} discarded due to high loss\")\n",
    "        if os.path.exists(inter_path):\n",
    "            os.remove(inter_path)\n",
    "    else:\n",
    "        with open(inter_path, \"wb\") as f:\n",
    "            pickle.dump(saver.intermediate_points, f)\n",
    "\n",
    "        metrics = get_loss_metrics()\n",
    "        nll, reg = float(metrics[\"nll_mean\"]), float(metrics[\"reg_mean\"])\n",
    "        final_lambda = float(current_reg_weight.numpy())\n",
    "        \n",
    "        MAX_NLL_TO_KEEP = 15.0\n",
    "        if nll > MAX_NLL_TO_KEEP:\n",
    "            print(f\"üóëÔ∏è Trial {trial_id} discarded due to high NLL: {nll:.2f}\")\n",
    "            os.remove(inter_path)\n",
    "            trial_info.update({\"final_nll\": nll, \"final_reg\": reg})\n",
    "        else:\n",
    "            val_loss_final = nll + final_lambda * reg\n",
    "            print(f\"‚úÖ Trial {trial_id} succeeded\")\n",
    "            trial_info.update({\n",
    "                \"valid_trial\": True,\n",
    "                \"final_nll\": nll,\n",
    "                \"final_reg\": reg,\n",
    "                \"final_val_loss\": val_loss_final\n",
    "            })\n",
    "\n",
    "    # Save csv\n",
    "    # Non-improving trials will NOT save their intermediate points & loss info on the csv \n",
    "    csv_path = os.path.join(exp_dir, \"info.csv\")\n",
    "    write_header = not os.path.exists(csv_path)\n",
    "\n",
    "    with open(csv_path, mode=\"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=trial_info.keys())\n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "        writer.writerow(trial_info)\n",
    "\n",
    "    return (trial_info[\"final_nll\"], trial_info[\"final_reg\"]) if trial_info[\"valid_trial\"] else (None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc34585-f62f-4e78-8eb8-fb0fa7ea84b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Trial runner \n",
    "def run_trials(scheduler_type, experiment_name, num_trials, epochs):\n",
    "    successful_trials = 0\n",
    "    trial_id = 0\n",
    "\n",
    "    while successful_trials < num_trials:\n",
    "        \n",
    "        # ==Sample hyperparameters (randomly selected)\n",
    "        # Modify the ranges you want so you can explore your scheduler's behavior.\n",
    "        # If you add a new scheduler and it has new parameters, be sure to introduce them here\n",
    "        \n",
    "        start_val = random.uniform(0.01, 3.0)\n",
    "        config = {\n",
    "            \"start\": start_val,\n",
    "            \"end\": random.uniform(start_val + 0.01, 8.0),\n",
    "            \"stop_threshold\": random.uniform(-40000.0, -20000.0),\n",
    "            \"max_epochs\": epochs\n",
    "        }\n",
    "        if scheduler_type == \"adaptive\":\n",
    "            config[\"step\"] = random.uniform(0.01, 0.2)\n",
    "            config[\"patience\"] = random.randint(3, 7)\n",
    "        elif scheduler_type == \"sigmoid\":\n",
    "            config[\"sharpness\"] = random.randint(2, 15)\n",
    "\n",
    "        print(f\"\\nüîÅ Training Trial {trial_id} using scheduler={scheduler_type}\")\n",
    "\n",
    "        lambda_init = config[\"start\"]\n",
    "        lambda_final = config.get(\"end\", None)\n",
    "        stop_threshold = config[\"stop_threshold\"]\n",
    "\n",
    "        nll, reg = objective_manual(\n",
    "            trial_id=trial_id,\n",
    "            lambda_init=lambda_init,\n",
    "            lambda_final=lambda_final,\n",
    "            stop_threshold=stop_threshold,\n",
    "            experiment_name=experiment_name,\n",
    "            scheduler_type=scheduler_type,\n",
    "            scheduler_kwargs=config,\n",
    "            epochs=epochs\n",
    "        )\n",
    "\n",
    "        if nll is not None:\n",
    "            successful_trials += 1\n",
    "\n",
    "        trial_id += 1\n",
    "\n",
    "# == Modify this block for each study you make \n",
    "# If you want very good pull/truth plots I recommend setting a lot of epochs ~ 500\n",
    "run_trials(\n",
    "    scheduler_type=\"cosine\",\n",
    "    experiment_name=\"general_test\",\n",
    "    num_trials=3, \n",
    "    epochs=500,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
