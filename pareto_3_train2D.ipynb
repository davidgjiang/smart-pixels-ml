{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a165585d-2561-4b2f-a98e-4c07d32f2aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === PARETO FRONT STUDIES NOTEBOOK #3 ===\n",
    "\n",
    "# This notebook helps train an specific best trial id with the conv2D model and it's corresponding loss scheduler.\n",
    "# This nb will generate a fingerprint with the training information and weights you can later plot on nb #4\n",
    "\n",
    "# == Before running:\n",
    "# Configure the same TFrecord paths you used in pareto_analysis.\n",
    "# Configure the intermediate_dir and experiment_name you used before.\n",
    "# I suggest checking out the best_trials.csv of your experiment and choosing the one you want to work with (based on loss, trayectory, stability...)\n",
    "# Once you determine the best trial you want to work with, set its id on the \"selected_trial_id\" variable.\n",
    "# Set the number of epochs you want on model.fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 01:01:07.129672: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-07-24 01:01:07.129752: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-07-24 01:01:07.130815: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-07-24 01:01:07.137799: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-07-24 01:01:08.103240: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from OptimizedDataGenerator_v2 import OptimizedDataGenerator\n",
    "import tensorflow_probability as tfp\n",
    "from models_16x16.models import *\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff499d81-383f-4988-94d7-12815179e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4567ea89-d140-41e4-a9c2-db26e68e1e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==TFrecords paths\n",
    "tfrecords_dir_train = \"/home/callea/TFrecords_3src_filtered/train\"\n",
    "tfrecords_dir_validation = \"/home/callea/TFrecords_3src_filtered/test\"\n",
    "\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train,\n",
    "    shuffle = True,\n",
    "    seed = 13,\n",
    "    quantize = True\n",
    ")\n",
    "\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_validation,\n",
    "    shuffle = True,\n",
    "    seed = 13,\n",
    "    quantize = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a218ba-cb22-429d-9cb9-20a76dd670ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss: NLL and a sum of standard deviations sigma regularizer (you can modify the regularizer).\n",
    "\n",
    "current_reg_weight = tf.Variable(0.0, trainable=False, dtype=tf.float32, name='reg_weight')\n",
    "\n",
    "def custom_loss(y, p_base, minval=1e-9, maxval=1e9, scale = 512):\n",
    "\n",
    "    reg_weight = current_reg_weight\n",
    "    \n",
    "    p = p_base\n",
    "    \n",
    "    mu = p[:, 0:8:2]\n",
    "    \n",
    "    # creating each matrix element in 4x4\n",
    "    Mdia = minval + tf.math.maximum(p[:, 1:8:2], 0.0)\n",
    "    Mcov = p[:,8:]\n",
    "    \n",
    "    # placeholder zero element\n",
    "    zeros = tf.zeros_like(Mdia[:,0])\n",
    "    \n",
    "    # assembles scale_tril matrix\n",
    "    row1 = tf.stack([Mdia[:,0],zeros,zeros,zeros])\n",
    "    row2 = tf.stack([Mcov[:,0],Mdia[:,1],zeros,zeros])\n",
    "    row3 = tf.stack([Mcov[:,1],Mcov[:,2],Mdia[:,2],zeros])\n",
    "    row4 = tf.stack([Mcov[:,3],Mcov[:,4],Mcov[:,5],Mdia[:,3]])\n",
    "\n",
    "    scale_tril = tf.transpose(tf.stack([row1,row2,row3,row4]),perm=[2,0,1])\n",
    "\n",
    "    dist = tfp.distributions.MultivariateNormalTriL(loc = mu, scale_tril = scale_tril) \n",
    "    \n",
    "    likelihood = dist.prob(y)  \n",
    "    likelihood = tf.clip_by_value(likelihood,minval,maxval)\n",
    "\n",
    "    NLL = -1*tf.math.log(likelihood)\n",
    "\n",
    "    cov_matrix = tf.matmul(scale_tril, tf.transpose(scale_tril, [0, 2, 1])) \n",
    "    variances = tf.linalg.diag_part(cov_matrix)\n",
    "    stds = tf.sqrt(variances + minval)\n",
    "\n",
    "    sigma_regularizer_1 = tf.reduce_sum(stds, axis=1)\n",
    "\n",
    "    batch_size = tf.shape(y)[0]\n",
    "    \n",
    "    track_loss_values(NLL, sigma_regularizer_1)\n",
    "\n",
    "    total_loss = NLL + (sigma_regularizer_1 * reg_weight)\n",
    "    \n",
    "    return tf.keras.backend.sum(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fd3960-c7f1-4d6c-b7e0-0c4c833797d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compiles model\n",
    "model = CreateModel((16,16,2), n_filters=5, pool_size=3)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b8741-98fd-4e1e-bc29-7b9fcbb9b640",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=1e-3), loss=custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60b630-5e1e-41cd-b9a0-7e9a06e9a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "base_dir = f'./trained_models/model-{fingerprint}-checkpoints'\n",
    "os.makedirs(base_dir, exist_ok=True)  \n",
    "checkpoint_filepath = base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccc68b0-0d5a-4b14-bebc-0d8e6923e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91025959-570b-4ede-aa69-c4dc5e577a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ==Important paths you need to configure!\n",
    "intermediate_dir = \"/home/callea/smart-pixels-ml/intermediate_logs\"\n",
    "experiment_name = \"general_test\"\n",
    "best_csv_path = os.path.join(intermediate_dir, experiment_name, \"best_trials.csv\")\n",
    "\n",
    "# Leer el CSV y mostrar opciones\n",
    "df = pd.read_csv(best_csv_path)\n",
    "print(\"Trial IDs disponibles en best_trials.csv:\")\n",
    "print(df[[\"trial_id\", \"keras_val_loss\"]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179570ab-e209-4a03-a38f-839860fb8764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==Select trial ID you want to train with the conv2D model\n",
    "selected_trial_id = 0\n",
    "row = df[df[\"trial_id\"] == selected_trial_id].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd97cbbf-a137-4a5c-9796-11de00abfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, Callback\n",
    "from schedulers import *\n",
    "early_stopping_patience = 50\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        checkpoints = [f for f in os.listdir(base_dir) if f.startswith('weights')]\n",
    "        if len(checkpoints) > 1:\n",
    "            checkpoints.sort()\n",
    "            for checkpoint in checkpoints[:-1]:\n",
    "                os.remove(os.path.join(base_dir, checkpoint))\n",
    "\n",
    "es = EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "mcp = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "scheduler_type = row[\"scheduler\"]\n",
    "current_reg_weight.assign(float(row[\"lambda_init\"])) \n",
    "\n",
    "def build_scheduler(row, reg_weight_var):\n",
    "    scheduler_type = row[\"scheduler\"]\n",
    "    kwargs = {\n",
    "        \"reg_weight_var\": reg_weight_var,\n",
    "        \"start\": float(row[\"lambda_init\"])\n",
    "    }\n",
    "\n",
    "    if scheduler_type in [\"cosine\", \"linear\", \"sigmoid\"]:\n",
    "        kwargs[\"end\"] = float(row[\"lambda_final\"])\n",
    "        kwargs[\"stop_threshold\"] = float(row[\"stop_threshold\"])\n",
    "        if scheduler_type == \"sigmoid\":\n",
    "            kwargs[\"sharpness\"] = float(row[\"sharpness\"]) \n",
    "    elif scheduler_type == \"adaptive\":\n",
    "        kwargs[\"step\"] = float(row[\"step\"])\n",
    "        kwargs[\"patience\"] = int(row[\"patience\"])\n",
    "\n",
    "    schedulers = {\n",
    "        \"cosine\": CosineScheduler,\n",
    "        \"linear\": LinearScheduler,\n",
    "        \"sigmoid\": SigmoidScheduler,\n",
    "        \"adaptive\": AdaptiveScheduler\n",
    "    }\n",
    "\n",
    "    if scheduler_type not in schedulers:\n",
    "        raise ValueError(f\"Unsupported scheduler type: {scheduler_type}\")\n",
    "\n",
    "    print(f\"Scheduler selected: {scheduler_type}\")\n",
    "    return schedulers[scheduler_type](**kwargs)\n",
    "\n",
    "scheduler = build_scheduler(row, current_reg_weight)\n",
    "\n",
    "\n",
    "csv_logger = CSVLogger(f'{base_dir}/training_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "        x=training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[es, mcp, csv_logger, scheduler],\n",
    "        epochs=1000,\n",
    "        shuffle=False,\n",
    "        steps_per_epoch=len(training_generator),\n",
    "        validation_steps=len(validation_generator),\n",
    "        verbose=1\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
