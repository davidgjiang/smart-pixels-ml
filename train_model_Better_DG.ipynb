{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 01:31:23.306084: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-10 01:31:23.306183: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-10 01:31:23.384683: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-06-10 01:31:23.548854: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-10 01:31:36.352149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08697ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/das214/SmartPix/BetterDG\n"
     ]
    }
   ],
   "source": [
    "# os.chdir('SmartPix/data_generator')\n",
    "os.chdir('SmartPix/BetterDG')\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05666e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "codesign_catapult.ipynb       preselection_processor.py\n",
      "docs\t\t\t      __pycache__\n",
      "evaluate.py\t\t      requirements.txt\n",
      "from_weights.ipynb\t      site\n",
      "loss.py\t\t\t      test_run.py\n",
      "Merge_plan_lab.ipynb\t      trained_models\n",
      "mergeplan.py\t\t      train_model_Better_DG.ipynb\n",
      "mkdocs.yml\t\t      train_model_new_example.ipynb\n",
      "model_batchnorm\t\t      train_model_optimized_giuseppe.ipynb\n",
      "models.py\t\t      train_model_optimized.ipynb\n",
      "OptimizedDataGeneratorNew.py  train_model.py\n",
      "OptimizedDataGenerator.py     utils.py\n",
      "OptimizedDataGenerator_v2.py  xysum_model.py\n",
      "plotting\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39ee1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import OptimizedDataGenerator_v2\n",
    "\n",
    "importlib.reload(OptimizedDataGenerator_v2)\n",
    "from OptimizedDataGenerator_v2 import OptimizedDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0170239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dataprep import *\n",
    "# from OptimizedDataGenerator_v2 import OptimizedDataGenerator\n",
    "from loss import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd94db45-10a3-4aa0-9885-cad5c6cc25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"/depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/\"\n",
    "tfrecords_base_dir = os.path.join(dataset_base_dir, \"TFR_files\", \"2t\")\n",
    "\n",
    "dataset_base_dir = os.path.join(dataset_base_dir, \"parquets\")\n",
    "tfrecords_dir_train = os.path.join(tfrecords_base_dir, \"TFR_train\")\n",
    "tfrecords_dir_val   = os.path.join(tfrecords_base_dir, \"TFR_val\")\n",
    "\n",
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = 75\n",
    "val_file_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dccba057-7aa7-4b85-95bd-8977c14cf7ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 25/25 [00:16<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_val is removed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving batches as TFRecords:   0%|          | 0/102 [00:00<?, ?it/s]2025-06-10 01:32:36.913999: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3234 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB MIG 1g.5gb, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "Saving batches as TFRecords: 100%|██████████| 102/102 [00:24<00:00,  4.15it/s]\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved successfully ast /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_val/metadata.json\n",
      "Loading metadata from /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_val/metadata.json\n",
      "--- Validation generator 41.25817394256592 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 75/75 [00:45<00:00,  1.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_train is removed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving batches as TFRecords: 100%|██████████| 306/306 [01:13<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved successfully ast /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_train/metadata.json\n",
      "Loading metadata from /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_train/metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training generator 120.27378225326538 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_base_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    # optimize_batch_size = True,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, \n",
    "    files_from_end=True,\n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_val,\n",
    "    use_time_stamps = [0,19],\n",
    "    max_workers = 2\n",
    ")\n",
    "\n",
    "print(\"--- Validation generator %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# training generator\n",
    "start_time = time.time()\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_base_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    # optimize_batch_size = True,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, # True \n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = [0,19],\n",
    "    max_workers = 2\n",
    ")\n",
    "print(\"--- Training generator %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9cbb689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_val/metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n",
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from /depot/cms/users/das214/datasets/dataset_2s/dataset_2s_50x12P5_parquets/TFR_files/2t/TFR_train/metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Loading pre-generated TFRecords\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir= tfrecords_dir_val,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    quantize=True,\n",
    ")\n",
    "\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    "    quantize=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72cef1b-61db-489f-83bb-039a46861094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-10 01:35:00.876649: I external/local_tsl/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 13, 21, 2)]       0         \n",
      "                                                                 \n",
      " q_separable_conv2d (QSepar  (None, 11, 19, 5)         33        \n",
      " ableConv2D)                                                     \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 11, 19, 5)         0         \n",
      "                                                                 \n",
      " q_conv2d (QConv2D)          (None, 11, 19, 5)         30        \n",
      "                                                                 \n",
      " q_activation_1 (QActivatio  (None, 11, 19, 5)         0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 3, 6, 5)           0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " q_activation_2 (QActivatio  (None, 3, 6, 5)           0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 90)                0         \n",
      "                                                                 \n",
      " q_dense (QDense)            (None, 16)                1456      \n",
      "                                                                 \n",
      " q_activation_3 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_1 (QDense)          (None, 16)                272       \n",
      "                                                                 \n",
      " q_activation_4 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_2 (QDense)          (None, 14)                238       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2029 (7.93 KB)\n",
      "Trainable params: 2029 (7.93 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=CreateModel((13,21,2),n_filters=5,pool_size=3)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Nadam(learning_rate=1e-3),\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce60b630-5e1e-41cd-b9a0-7e9a06e9a395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "timestamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "os.makedirs(\"trained_models\", exist_ok=True)\n",
    "base_dir = f'./trained_models/model-{fingerprint}-checkpoints'\n",
    "os.makedirs(base_dir, exist_ok=True)  \n",
    "checkpoint_filepath = base_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ccc68b0-0d5a-4b14-bebc-0d8e6923e178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3f76ef3f\n"
     ]
    }
   ],
   "source": [
    "print(fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd97cbbf-a137-4a5c-9796-11de00abfa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger, EarlyStopping, ModelCheckpoint, Callback\n",
    "\n",
    "early_stopping_patience = 50\n",
    "\n",
    "class CustomModelCheckpoint(ModelCheckpoint):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "        checkpoints = [f for f in os.listdir(base_dir) if f.startswith('weights')]\n",
    "        if len(checkpoints) > 1:\n",
    "            checkpoints.sort()\n",
    "            for checkpoint in checkpoints[:-1]:\n",
    "                os.remove(os.path.join(base_dir, checkpoint))\n",
    "\n",
    "es = EarlyStopping(patience=early_stopping_patience, restore_best_weights=True)\n",
    "\n",
    "mcp = CustomModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    save_freq='epoch',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(f'{base_dir}/training_log.csv', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "        x=training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[es, mcp, csv_logger],\n",
    "        epochs=1000,\n",
    "        shuffle=False,\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4e2706-3e27-4ece-9ebd-be135f6bbcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Epoch 1/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: 27173.7227\n",
    "Epoch 1: val_loss improved from inf to 3957.69629, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.01-t27173.72-v3957.70.hdf5\n",
    "305/305 [==============================] - 34s 102ms/step - loss: 27173.7227 - val_loss: 3957.6963\n",
    "Epoch 2/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: 2077.7952\n",
    "Epoch 2: val_loss improved from 3957.69629 to -473.36844, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.02-t2077.80-v-473.37.hdf5\n",
    "305/305 [==============================] - 30s 97ms/step - loss: 2077.7952 - val_loss: -473.3684\n",
    "Epoch 3/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: -935.6221\n",
    "Epoch 3: val_loss improved from -473.36844 to -1284.97668, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.03-t-935.62-v-1284.98.hdf5\n",
    "305/305 [==============================] - 61s 199ms/step - loss: -935.6221 - val_loss: -1284.9767\n",
    "Epoch 4/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: -484.1406\n",
    "Epoch 4: val_loss improved from -1284.97668 to -1554.20630, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.04-t-484.14-v-1554.21.hdf5\n",
    "305/305 [==============================] - 36s 116ms/step - loss: -484.1406 - val_loss: -1554.2063\n",
    "Epoch 5/1000\n",
    "305/305 [==============================] - ETA: 0s - loss: -3135.4961\n",
    "Epoch 5: val_loss improved from -1554.20630 to -2936.62402, saving model to ./trained_models/model-e0ef8b33-checkpoints/weights.05-t-3135.50-v-2936.62.hdf5\n",
    "305/305 [==============================] - 40s 130ms/step - loss: -3135.4961 - val_loss: -2936.6240\n",
    "Epoch 6/1000\n",
    " 38/305 [==>...........................] - ETA: 19s - loss: -3885.3333\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8866bd46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fb1ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5a6d94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 kernel (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
