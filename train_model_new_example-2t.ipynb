{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 18:45:12.498997: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-06-17 18:45:13.449144: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac065e8-fcf2-44a0-8dee-e1e44e605137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.OptimizedDataGenerator_v2 import OptimizedDataGenerator\n",
    "from loss import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80abd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbc64d0-998a-4022-b4f5-e886c4c07941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"/data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/\"\n",
    "tfrecords_base_dir = os.path.join(dataset_base_dir, \"TFR_files\", \"2t\")\n",
    "\n",
    "dataset_train_dir = os.path.join(dataset_base_dir, \"train\")\n",
    "dataset_validation_dir = os.path.join(dataset_base_dir, \"test\")\n",
    "tfrecords_dir_train = os.path.join(tfrecords_base_dir, \"TFR_train\")\n",
    "tfrecords_dir_val   = os.path.join(tfrecords_base_dir, \"TFR_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c17a48-9308-40e4-a126-5478659cfedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 80\n",
      "Number of validation files: 20\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training files: {len(os.listdir(dataset_train_dir))}')\n",
    "print(f'Number of validation files: {len(os.listdir(dataset_validation_dir))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8bb9b7-c75e-489d-83bc-ad517c652aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = len(os.listdir(dataset_train_dir))\n",
    "val_file_size = len(os.listdir(dataset_validation_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b6afa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 20/20 [00:04<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/TFR_files/2t/TFR_val is removed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving batches as TFRecords:   0%|          | 0/21 [00:00<?, ?it/s]2025-06-17 18:45:35.892511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3234 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 1g.5gb, pci bus id: 0000:81:00.0, compute capability: 8.0\n",
      "Saving batches as TFRecords: 100%|██████████| 21/21 [00:07<00:00,  2.90it/s]\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved successfully ast /data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/TFR_files/2t/TFR_val/metadata.json\n",
      "Loading metadata from /data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/TFR_files/2t/TFR_val/metadata.json\n",
      "--- Validation generator 12.389603614807129 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_validation_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,16,16), # (20,16,16),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, \n",
    "    files_from_end=True,\n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_val,\n",
    "    use_time_stamps = [0,19],\n",
    "    max_workers = 2,\n",
    "    #load_from_tfrecords_dir = tfrecords_dir_val\n",
    ")\n",
    "\n",
    "print(\"--- Validation generator %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548efe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files...: 100%|██████████| 80/80 [00:18<00:00,  4.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory /data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/TFR_files/2t/TFR_train is removed...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving batches as TFRecords: 100%|██████████| 84/84 [00:27<00:00,  3.08it/s]\n",
      "WARNING:root:Quantization is False in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved successfully ast /data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/TFR_files/2t/TFR_train/metadata.json\n",
      "Loading metadata from /data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/TFR_files/2t/TFR_train/metadata.json\n",
      "--- Training generator 46.41184854507446 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# training generator\n",
    "start_time = time.time()\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_train_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (2,16,16), # (20,16,16),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, # True \n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = [0,19],\n",
    "    max_workers = 2,\n",
    "    #load_from_tfrecords_dir = tfrecords_dir_train\n",
    "\n",
    ")\n",
    "print(\"--- Training generator %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "248e5d17-2a8f-4e8c-871a-65a0ab3794b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from /data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/TFR_files/2t/TFR_train/metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n",
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading metadata from /data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_16x16_50x12P5_parquets/contained/TFR_files/2t/TFR_val/metadata.json\n"
     ]
    }
   ],
   "source": [
    "training_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train,\n",
    "    shuffle = True,\n",
    "    seed = seed,\n",
    "    quantize = True\n",
    ")\n",
    "\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_val,\n",
    "    shuffle = True,\n",
    "    seed = seed,\n",
    "    quantize = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c72cef1b-61db-489f-83bb-039a46861094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 16, 16, 2)]       0         \n",
      "                                                                 \n",
      " q_separable_conv2d (QSepar  (None, 14, 14, 5)         33        \n",
      " ableConv2D)                                                     \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 14, 14, 5)         0         \n",
      "                                                                 \n",
      " q_conv2d (QConv2D)          (None, 14, 14, 5)         30        \n",
      "                                                                 \n",
      " q_activation_1 (QActivatio  (None, 14, 14, 5)         0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 4, 4, 5)           0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " q_activation_2 (QActivatio  (None, 4, 4, 5)           0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 80)                0         \n",
      "                                                                 \n",
      " q_dense (QDense)            (None, 16)                1296      \n",
      "                                                                 \n",
      " q_activation_3 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_1 (QDense)          (None, 16)                272       \n",
      "                                                                 \n",
      " q_activation_4 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_2 (QDense)          (None, 14)                238       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1869 (7.30 KB)\n",
      "Trainable params: 1869 (7.30 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=CreateModel((16,16,2),n_filters=5,pool_size=3)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "641b38fa-0cfb-4e17-859f-6570c889d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fingerprint: 34c2da80\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "pitch = '50x12P5'\n",
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "base_dir = '/home/dajiang/smart-pixels-ml/weights/dataset_3src_16x16_weights/'\n",
    "weights_dir = base_dir + 'weights-{}-bs{}-{}-2t-checkpoints'.format(pitch, batch_size, fingerprint)\n",
    "\n",
    "# create output directories\n",
    "if os.path.isdir(base_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "else:\n",
    "    os.mkdir(base_dir)\n",
    "    os.mkdir(weights_dir)\n",
    "    \n",
    "checkpoint_filepath = weights_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'\n",
    "mcp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=False,\n",
    ")\n",
    "\n",
    "print('Model fingerprint: {}'.format(fingerprint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-17 18:46:32.197636: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8906\n",
      "2025-06-17 18:46:32.312625: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-06-17 18:46:32.552624: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-06-17 18:46:32.558756: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0xb376ac0\n",
      "2025-06-17 18:46:32.578990: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5df0e4cb90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-06-17 18:46:32.579024: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 1g.5gb, Compute Capability 8.0\n",
      "2025-06-17 18:46:32.583064: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-06-17 18:46:32.635730: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-06-17 18:46:32.682927: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 19s 178ms/step - loss: 62272.3984 - val_loss: 16089.8379\n",
      "Epoch 2/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: 13463.6221 - val_loss: 10952.5928\n",
      "Epoch 3/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: 8779.8193 - val_loss: 6302.3794\n",
      "Epoch 4/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: 4569.5757 - val_loss: 2709.1526\n",
      "Epoch 5/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: 1710.5869 - val_loss: 230.3340\n",
      "Epoch 6/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -890.5454 - val_loss: -1906.1260\n",
      "Epoch 7/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -2578.3140 - val_loss: -3366.5847\n",
      "Epoch 8/1000\n",
      "84/84 [==============================] - 6s 77ms/step - loss: -3990.9927 - val_loss: -4335.8901\n",
      "Epoch 9/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -3260.2598 - val_loss: -4234.2295\n",
      "Epoch 10/1000\n",
      "84/84 [==============================] - 7s 77ms/step - loss: 27590.0273 - val_loss: -5561.5005\n",
      "Epoch 11/1000\n",
      "84/84 [==============================] - 6s 77ms/step - loss: -4696.8130 - val_loss: -6434.2056\n",
      "Epoch 12/1000\n",
      "84/84 [==============================] - 6s 76ms/step - loss: -6020.3931 - val_loss: -7363.8047\n",
      "Epoch 13/1000\n",
      "84/84 [==============================] - 6s 76ms/step - loss: -4696.6245 - val_loss: -7654.0010\n",
      "Epoch 14/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -6460.9160 - val_loss: -7804.8145\n",
      "Epoch 15/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -6767.7134 - val_loss: -8240.1621\n",
      "Epoch 16/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -3502.0649 - val_loss: -5107.8760\n",
      "Epoch 17/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -7657.8423 - val_loss: -8234.4014\n",
      "Epoch 18/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -8608.4717 - val_loss: -8984.7734\n",
      "Epoch 19/1000\n",
      "84/84 [==============================] - 7s 77ms/step - loss: -8960.4717 - val_loss: -8545.7627\n",
      "Epoch 20/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -7600.8389 - val_loss: -835.7683\n",
      "Epoch 21/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -9076.5732 - val_loss: -10067.2607\n",
      "Epoch 22/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -8943.2666 - val_loss: -7970.2363\n",
      "Epoch 23/1000\n",
      "84/84 [==============================] - 7s 85ms/step - loss: -7067.0557 - val_loss: -9771.0322\n",
      "Epoch 24/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -9821.1055 - val_loss: -9983.0479\n",
      "Epoch 25/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -10119.7178 - val_loss: -10310.5771\n",
      "Epoch 26/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -10465.0293 - val_loss: -10721.0459\n",
      "Epoch 27/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -10706.8320 - val_loss: -10923.0068\n",
      "Epoch 28/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -10775.3242 - val_loss: -10651.7842\n",
      "Epoch 29/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -8553.5596 - val_loss: -6708.2729\n",
      "Epoch 30/1000\n",
      "84/84 [==============================] - 7s 77ms/step - loss: -9216.4717 - val_loss: -10457.7021\n",
      "Epoch 31/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -10889.1689 - val_loss: -11212.1299\n",
      "Epoch 32/1000\n",
      "84/84 [==============================] - 7s 85ms/step - loss: -10012.5234 - val_loss: -7935.0684\n",
      "Epoch 33/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -8701.7861 - val_loss: -9080.4922\n",
      "Epoch 34/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -10591.2852 - val_loss: -10825.3379\n",
      "Epoch 35/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -10890.4395 - val_loss: -10717.3477\n",
      "Epoch 36/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -11007.7354 - val_loss: -11453.2266\n",
      "Epoch 37/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -11372.2148 - val_loss: -11820.2568\n",
      "Epoch 38/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -11355.8613 - val_loss: -11448.9590\n",
      "Epoch 39/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -11379.7461 - val_loss: -11533.6562\n",
      "Epoch 40/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -11508.9834 - val_loss: -12012.5322\n",
      "Epoch 41/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -11269.7334 - val_loss: -7202.4629\n",
      "Epoch 42/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -10972.2051 - val_loss: -12002.8916\n",
      "Epoch 43/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -9464.1699 - val_loss: -11387.1553\n",
      "Epoch 44/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -11524.9512 - val_loss: -12003.1328\n",
      "Epoch 45/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -11229.7002 - val_loss: -12096.7422\n",
      "Epoch 46/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -12114.8252 - val_loss: -12113.8125\n",
      "Epoch 47/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12156.6064 - val_loss: -11729.2256\n",
      "Epoch 48/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -12031.8584 - val_loss: -10758.7959\n",
      "Epoch 49/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -11768.6396 - val_loss: -12261.5010\n",
      "Epoch 50/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -11299.8008 - val_loss: -11546.5664\n",
      "Epoch 51/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -12008.3994 - val_loss: -12306.5010\n",
      "Epoch 52/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -12185.5459 - val_loss: -12058.7793\n",
      "Epoch 53/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -11116.1885 - val_loss: -10257.9990\n",
      "Epoch 54/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -12141.7969 - val_loss: -12370.5420\n",
      "Epoch 55/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12696.2002 - val_loss: -12849.4307\n",
      "Epoch 56/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -12942.8467 - val_loss: -13153.0342\n",
      "Epoch 57/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -13061.0625 - val_loss: -13060.8330\n",
      "Epoch 58/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -13181.4922 - val_loss: -13117.3896\n",
      "Epoch 59/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12850.5693 - val_loss: -12243.1680\n",
      "Epoch 60/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12993.2939 - val_loss: -12906.7148\n",
      "Epoch 61/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -13330.0537 - val_loss: -13353.7842\n",
      "Epoch 62/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -13408.7715 - val_loss: -13296.6445\n",
      "Epoch 63/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -13387.4629 - val_loss: -13483.6221\n",
      "Epoch 64/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13433.6465 - val_loss: -13604.7646\n",
      "Epoch 65/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13442.7812 - val_loss: -12919.6152\n",
      "Epoch 66/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -11539.9902 - val_loss: -10258.3086\n",
      "Epoch 67/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -12234.1641 - val_loss: -12956.9326\n",
      "Epoch 68/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13195.2383 - val_loss: -13455.2109\n",
      "Epoch 69/1000\n",
      "84/84 [==============================] - 7s 86ms/step - loss: -13312.5723 - val_loss: -12851.8535\n",
      "Epoch 70/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -13216.5908 - val_loss: -13341.4531\n",
      "Epoch 71/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13267.6768 - val_loss: -12503.4834\n",
      "Epoch 72/1000\n",
      "84/84 [==============================] - 7s 86ms/step - loss: -12961.1885 - val_loss: -11572.3057\n",
      "Epoch 73/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -12875.8838 - val_loss: -7657.1147\n",
      "Epoch 74/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -10821.5967 - val_loss: -11493.9355\n",
      "Epoch 75/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12555.0889 - val_loss: -12124.5225\n",
      "Epoch 76/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -12605.3750 - val_loss: -12444.9316\n",
      "Epoch 77/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -12823.5664 - val_loss: -12521.4473\n",
      "Epoch 78/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -12734.9951 - val_loss: -11311.3047\n",
      "Epoch 79/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12747.0566 - val_loss: -13402.4414\n",
      "Epoch 80/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13253.3701 - val_loss: -13048.5273\n",
      "Epoch 81/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -12642.7080 - val_loss: -13412.8545\n",
      "Epoch 82/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12260.8203 - val_loss: -13126.7646\n",
      "Epoch 83/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -12578.4814 - val_loss: -13302.7529\n",
      "Epoch 84/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -12413.7207 - val_loss: -13519.5254\n",
      "Epoch 85/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -12436.6455 - val_loss: -13388.5654\n",
      "Epoch 86/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -12655.0322 - val_loss: -13187.4092\n",
      "Epoch 87/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -11847.3975 - val_loss: -12601.2568\n",
      "Epoch 88/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -12783.5879 - val_loss: -13113.5479\n",
      "Epoch 89/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12543.0811 - val_loss: -12974.5234\n",
      "Epoch 90/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -12662.3408 - val_loss: -13270.3389\n",
      "Epoch 91/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -12342.2109 - val_loss: -12458.4707\n",
      "Epoch 92/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13214.7510 - val_loss: -12677.7539\n",
      "Epoch 93/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -12789.1543 - val_loss: -13059.0234\n",
      "Epoch 94/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -13051.8740 - val_loss: -13280.6582\n",
      "Epoch 95/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13088.9453 - val_loss: -14018.9883\n",
      "Epoch 96/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -13316.7207 - val_loss: -10489.5557\n",
      "Epoch 97/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -13458.2324 - val_loss: -14271.9834\n",
      "Epoch 98/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13783.1387 - val_loss: -14244.4941\n",
      "Epoch 99/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -14091.8516 - val_loss: -14348.8018\n",
      "Epoch 100/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -14385.6426 - val_loss: -14758.5293\n",
      "Epoch 101/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -13856.3535 - val_loss: -14688.7539\n",
      "Epoch 102/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -14308.6504 - val_loss: -14979.5420\n",
      "Epoch 103/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -14439.4365 - val_loss: -15044.6465\n",
      "Epoch 104/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -14436.8320 - val_loss: -14398.9473\n",
      "Epoch 105/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -14890.2725 - val_loss: -15027.3330\n",
      "Epoch 106/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -14682.3486 - val_loss: -15135.4053\n",
      "Epoch 107/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -14969.0986 - val_loss: -15440.3115\n",
      "Epoch 108/1000\n",
      "84/84 [==============================] - 7s 87ms/step - loss: -15240.4609 - val_loss: -15547.7803\n",
      "Epoch 109/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15375.0244 - val_loss: -15464.7129\n",
      "Epoch 110/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15444.1719 - val_loss: -15627.4395\n",
      "Epoch 111/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15588.1104 - val_loss: -15826.9951\n",
      "Epoch 112/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15622.2266 - val_loss: -15862.5586\n",
      "Epoch 113/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15651.4443 - val_loss: -16154.6543\n",
      "Epoch 114/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -15645.1631 - val_loss: -15659.4561\n",
      "Epoch 115/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -15927.7910 - val_loss: -14579.3027\n",
      "Epoch 116/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -15790.6836 - val_loss: -16053.9053\n",
      "Epoch 117/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15416.5811 - val_loss: -16192.8916\n",
      "Epoch 118/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -15280.8994 - val_loss: -14578.4102\n",
      "Epoch 119/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -14696.6465 - val_loss: -14728.0488\n",
      "Epoch 120/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -14836.0059 - val_loss: -15379.3525\n",
      "Epoch 121/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -14990.3809 - val_loss: -15485.1885\n",
      "Epoch 122/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -15522.6709 - val_loss: -14576.5400\n",
      "Epoch 123/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -15440.3271 - val_loss: -15964.0439\n",
      "Epoch 124/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15458.3174 - val_loss: -15667.4912\n",
      "Epoch 125/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -15721.9844 - val_loss: -16113.1592\n",
      "Epoch 126/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15743.5850 - val_loss: -15936.6191\n",
      "Epoch 127/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -15828.3848 - val_loss: -15678.0986\n",
      "Epoch 128/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15854.7168 - val_loss: -15403.4736\n",
      "Epoch 129/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15567.2734 - val_loss: -14415.4971\n",
      "Epoch 130/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15325.8359 - val_loss: -16090.1484\n",
      "Epoch 131/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15572.9785 - val_loss: -12818.4873\n",
      "Epoch 132/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15332.6631 - val_loss: -15823.0020\n",
      "Epoch 133/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15561.2559 - val_loss: -16146.5439\n",
      "Epoch 134/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15162.6885 - val_loss: -16061.5518\n",
      "Epoch 135/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -15533.4639 - val_loss: -14954.5068\n",
      "Epoch 136/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15563.6465 - val_loss: -15909.2119\n",
      "Epoch 137/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15385.0557 - val_loss: -16040.5146\n",
      "Epoch 138/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -12873.4219 - val_loss: -15062.4209\n",
      "Epoch 139/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15837.5654 - val_loss: -16260.7764\n",
      "Epoch 140/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -16076.0020 - val_loss: -16489.9590\n",
      "Epoch 141/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -16182.3799 - val_loss: -16536.4961\n",
      "Epoch 142/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -15436.0947 - val_loss: -14114.2773\n",
      "Epoch 143/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -14960.8877 - val_loss: -15371.6494\n",
      "Epoch 144/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -15016.0547 - val_loss: -15481.7988\n",
      "Epoch 145/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15229.3906 - val_loss: -12110.9590\n",
      "Epoch 146/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -14782.1621 - val_loss: -15997.8955\n",
      "Epoch 147/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -15518.2588 - val_loss: -15439.0684\n",
      "Epoch 148/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15246.8066 - val_loss: -16353.3291\n",
      "Epoch 149/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15209.0107 - val_loss: -15980.3096\n",
      "Epoch 150/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -15447.1318 - val_loss: -15117.5186\n",
      "Epoch 151/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -15630.2852 - val_loss: -15547.8867\n",
      "Epoch 152/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -16117.1680 - val_loss: -10788.4766\n",
      "Epoch 153/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -16229.2598 - val_loss: -16853.9258\n",
      "Epoch 154/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16481.8203 - val_loss: -16892.8984\n",
      "Epoch 155/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -16739.3789 - val_loss: -16579.2461\n",
      "Epoch 156/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16345.9834 - val_loss: -16614.9258\n",
      "Epoch 157/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17184.9336 - val_loss: -17310.3516\n",
      "Epoch 158/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -16981.4258 - val_loss: -15798.4521\n",
      "Epoch 159/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16897.0391 - val_loss: -17238.4941\n",
      "Epoch 160/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -15962.7100 - val_loss: -17255.4551\n",
      "Epoch 161/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16639.3379 - val_loss: -17250.2207\n",
      "Epoch 162/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -16869.3750 - val_loss: -17290.9668\n",
      "Epoch 163/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -16587.2090 - val_loss: -16660.6094\n",
      "Epoch 164/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16887.5391 - val_loss: -17039.7812\n",
      "Epoch 165/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17377.2188 - val_loss: -17247.9258\n",
      "Epoch 166/1000\n",
      "84/84 [==============================] - 7s 84ms/step - loss: -16446.8008 - val_loss: -17603.0020\n",
      "Epoch 167/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17310.6895 - val_loss: -15929.1689\n",
      "Epoch 168/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -16687.0254 - val_loss: -16391.0801\n",
      "Epoch 169/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16992.0078 - val_loss: -16831.9863\n",
      "Epoch 170/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16289.1221 - val_loss: -16167.6445\n",
      "Epoch 171/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17243.2402 - val_loss: -17930.1621\n",
      "Epoch 172/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17198.1387 - val_loss: -17148.6211\n",
      "Epoch 173/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16246.9004 - val_loss: -14695.8535\n",
      "Epoch 174/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16573.9961 - val_loss: -17605.0566\n",
      "Epoch 175/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17575.2422 - val_loss: -17182.5566\n",
      "Epoch 176/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -16566.6094 - val_loss: -16937.6855\n",
      "Epoch 177/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16211.6455 - val_loss: -16722.1270\n",
      "Epoch 178/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16012.7227 - val_loss: -16546.0000\n",
      "Epoch 179/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -15931.1719 - val_loss: -15045.7705\n",
      "Epoch 180/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15926.8857 - val_loss: -15580.1738\n",
      "Epoch 181/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -16541.1836 - val_loss: -16910.2891\n",
      "Epoch 182/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -17079.2539 - val_loss: -18217.9102\n",
      "Epoch 183/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -15282.4795 - val_loss: -17271.0918\n",
      "Epoch 184/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -17322.2578 - val_loss: -17442.4688\n",
      "Epoch 185/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17511.0254 - val_loss: -17726.4902\n",
      "Epoch 186/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17041.1035 - val_loss: -14456.0674\n",
      "Epoch 187/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -16945.7656 - val_loss: -17258.8438\n",
      "Epoch 188/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -17001.1875 - val_loss: -17447.6660\n",
      "Epoch 189/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17189.6973 - val_loss: -17576.3340\n",
      "Epoch 190/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17189.1758 - val_loss: -18175.8066\n",
      "Epoch 191/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17529.5684 - val_loss: -17742.3535\n",
      "Epoch 192/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17529.0117 - val_loss: -18363.6738\n",
      "Epoch 193/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -17871.7207 - val_loss: -17336.4121\n",
      "Epoch 194/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17441.9844 - val_loss: -13670.8213\n",
      "Epoch 195/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -17631.8770 - val_loss: -18110.8145\n",
      "Epoch 196/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17017.4258 - val_loss: -16449.9180\n",
      "Epoch 197/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17243.2695 - val_loss: -17717.5566\n",
      "Epoch 198/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17353.7676 - val_loss: -18028.0020\n",
      "Epoch 199/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -16871.8535 - val_loss: -17733.8535\n",
      "Epoch 200/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -17015.8125 - val_loss: -13018.3555\n",
      "Epoch 201/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -16981.2383 - val_loss: -17300.9648\n",
      "Epoch 202/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17706.8125 - val_loss: -17473.0625\n",
      "Epoch 203/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -17195.7754 - val_loss: -17555.4258\n",
      "Epoch 204/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -14615.8066 - val_loss: -18006.0820\n",
      "Epoch 205/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17154.7949 - val_loss: -18343.8516\n",
      "Epoch 206/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17478.0234 - val_loss: -18276.6035\n",
      "Epoch 207/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -16492.5410 - val_loss: -16134.0176\n",
      "Epoch 208/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17432.8809 - val_loss: -17233.5820\n",
      "Epoch 209/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17131.8945 - val_loss: -16703.5957\n",
      "Epoch 210/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17248.9141 - val_loss: -15572.7256\n",
      "Epoch 211/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -17254.4492 - val_loss: -17130.2188\n",
      "Epoch 212/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17753.1348 - val_loss: -17937.9277\n",
      "Epoch 213/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -18013.2852 - val_loss: -18178.1660\n",
      "Epoch 214/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -18177.3281 - val_loss: -16160.0283\n",
      "Epoch 215/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -17539.8672 - val_loss: -17008.2734\n",
      "Epoch 216/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17481.0645 - val_loss: -18010.6348\n",
      "Epoch 217/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -17506.1973 - val_loss: -16771.4844\n",
      "Epoch 218/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17393.0664 - val_loss: -17935.3789\n",
      "Epoch 219/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17688.3164 - val_loss: -17958.5312\n",
      "Epoch 220/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17240.9121 - val_loss: -17360.2617\n",
      "Epoch 221/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17237.4531 - val_loss: -18507.7773\n",
      "Epoch 222/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17235.2676 - val_loss: -17578.2324\n",
      "Epoch 223/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -17241.3320 - val_loss: -17166.4297\n",
      "Epoch 224/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -17386.5098 - val_loss: -16741.8379\n",
      "Epoch 225/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -17800.5703 - val_loss: -17946.8184\n",
      "Epoch 226/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17877.8066 - val_loss: -18175.5781\n",
      "Epoch 227/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -17780.8145 - val_loss: -16582.8438\n",
      "Epoch 228/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -17167.1426 - val_loss: -17729.6523\n",
      "Epoch 229/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -17834.3457 - val_loss: -15745.7793\n",
      "Epoch 230/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -18125.6895 - val_loss: -18605.0273\n",
      "Epoch 231/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -18247.0684 - val_loss: -18071.0430\n",
      "Epoch 232/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -18427.5840 - val_loss: -19647.8770\n",
      "Epoch 233/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -18380.5840 - val_loss: -18436.6641\n",
      "Epoch 234/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -17885.8105 - val_loss: -18370.0762\n",
      "Epoch 235/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -18473.5195 - val_loss: -18317.8262\n",
      "Epoch 236/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -18247.0996 - val_loss: -18572.0625\n",
      "Epoch 237/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -18542.3828 - val_loss: -16665.2422\n",
      "Epoch 238/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -18532.9297 - val_loss: -19430.2969\n",
      "Epoch 239/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -18710.5332 - val_loss: -18881.2227\n",
      "Epoch 240/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -18622.5488 - val_loss: -14239.5859\n",
      "Epoch 241/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -18880.7559 - val_loss: -19066.0527\n",
      "Epoch 242/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -18733.4844 - val_loss: -18579.4043\n",
      "Epoch 243/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -19114.8711 - val_loss: -17699.1309\n",
      "Epoch 244/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -19079.8789 - val_loss: -18558.6230\n",
      "Epoch 245/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -18670.5938 - val_loss: -19015.5762\n",
      "Epoch 246/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -18861.4414 - val_loss: -18648.0234\n",
      "Epoch 247/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -19048.9512 - val_loss: -18533.0098\n",
      "Epoch 248/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -19029.8203 - val_loss: -19144.0742\n",
      "Epoch 249/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -18480.0859 - val_loss: -18947.1660\n",
      "Epoch 250/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -18736.7598 - val_loss: -19519.8750\n",
      "Epoch 251/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -19033.2188 - val_loss: -19774.3906\n",
      "Epoch 252/1000\n",
      "84/84 [==============================] - 7s 83ms/step - loss: -18953.3125 - val_loss: -19406.9805\n",
      "Epoch 253/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -19101.8438 - val_loss: -19248.5684\n",
      "Epoch 254/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -19002.0059 - val_loss: -19544.9688\n",
      "Epoch 255/1000\n",
      "84/84 [==============================] - 7s 82ms/step - loss: -19265.4785 - val_loss: -19987.7227\n",
      "Epoch 256/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -19136.1113 - val_loss: -19499.4160\n",
      "Epoch 257/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -19251.9492 - val_loss: -18741.7754\n",
      "Epoch 258/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -19274.8828 - val_loss: -19526.1641\n",
      "Epoch 259/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -19326.9199 - val_loss: -19353.1855\n",
      "Epoch 260/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -19184.2988 - val_loss: -19591.6621\n",
      "Epoch 261/1000\n",
      "84/84 [==============================] - 7s 81ms/step - loss: -19320.0254 - val_loss: -19555.4141\n",
      "Epoch 262/1000\n",
      "84/84 [==============================] - 7s 78ms/step - loss: -19234.0781 - val_loss: -19734.1191\n",
      "Epoch 263/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -19617.3418 - val_loss: -19768.2363\n",
      "Epoch 264/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -19695.8984 - val_loss: -19657.0762\n",
      "Epoch 265/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -19819.9707 - val_loss: -19973.9531\n",
      "Epoch 266/1000\n",
      "84/84 [==============================] - 7s 79ms/step - loss: -19823.3516 - val_loss: -19984.8828\n",
      "Epoch 267/1000\n",
      "84/84 [==============================] - 7s 80ms/step - loss: -20018.8457 - val_loss: -20085.0547\n",
      "Epoch 268/1000\n",
      "34/84 [===========>..................] - ETA: 3s - loss: -20041.0996"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=[mcp],\n",
    "                    epochs=1000,\n",
    "                    shuffle=False, # shuffling now occurs within the data-loader\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06b616-1cfc-437c-9a18-836bcb218b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
