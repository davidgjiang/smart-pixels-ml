{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24773cdc-4bbe-48c3-9910-8b39c38bfc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 02:49:34.963822: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-13 02:49:35.879972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.utils import Sequence\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from qkeras import *\n",
    "\n",
    "from keras.utils import Sequence\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "pi = 3.14159265359\n",
    "\n",
    "maxval=1e9\n",
    "minval=1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ac065e8-fcf2-44a0-8dee-e1e44e605137",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloaders.OptimizedDataGeneratorNew import OptimizedDataGenerator\n",
    "from loss import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80abd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbc64d0-998a-4022-b4f5-e886c4c07941",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_base_dir = \"/data/dajiang/smart-pixels/dataset_3sr/shuffled/dataset_3sr_50x12P5_parquets/contained/recon3D\"\n",
    "tfrecords_base_dir = os.path.join(dataset_base_dir, \"TFR_files\", \"20t\")\n",
    "\n",
    "dataset_train_dir = os.path.join(dataset_base_dir, \"train\")\n",
    "dataset_validation_dir = os.path.join(dataset_base_dir, \"validation\")\n",
    "tfrecords_dir_train = os.path.join(tfrecords_base_dir, \"TFR_train\")\n",
    "tfrecords_dir_val   = os.path.join(tfrecords_base_dir, \"TFR_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80c17a48-9308-40e4-a126-5478659cfedd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 80\n",
      "Number of validation files: 20\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training files: {len(os.listdir(dataset_train_dir))}')\n",
    "print(f'Number of validation files: {len(os.listdir(dataset_validation_dir))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af8bb9b7-c75e-489d-83bc-ad517c652aab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 5000\n",
    "val_batch_size = 5000\n",
    "train_file_size = len(os.listdir(dataset_train_dir))\n",
    "val_file_size = len(os.listdir(dataset_validation_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b6afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_base_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = val_batch_size,\n",
    "    file_count = val_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (20,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, \n",
    "    files_from_end=True,\n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_val,\n",
    "    use_time_stamps = -1\n",
    "    max_workers = 2\n",
    ")\n",
    "\n",
    "print(\"--- Validation generator %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548efe18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training generator\n",
    "start_time = time.time()\n",
    "training_generator = OptimizedDataGenerator(\n",
    "    dataset_base_dir = dataset_base_dir,\n",
    "    file_type = \"parquet\",\n",
    "    data_format = \"3D\",\n",
    "    batch_size = batch_size,\n",
    "    file_count = train_file_size,\n",
    "    to_standardize= True,\n",
    "    include_y_local= False,\n",
    "    labels_list = ['x-midplane','y-midplane','cotAlpha','cotBeta'],\n",
    "    input_shape = (20,13,21), # (20,13,21),\n",
    "    transpose = (0,2,3,1),\n",
    "    shuffle = False, # True \n",
    "\n",
    "    tfrecords_dir = tfrecords_dir_train,\n",
    "    use_time_stamps = -1\n",
    "    max_workers = 2\n",
    ")\n",
    "print(\"--- Training generator %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248e5d17-2a8f-4e8c-871a-65a0ab3794b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n",
      "WARNING:root:Quantization is True in data generator. This may affect model performance.\n"
     ]
    }
   ],
   "source": [
    "training_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_train,\n",
    "    shuffle = True,\n",
    "    seed = seed,\n",
    "    quantize = True\n",
    ")\n",
    "\n",
    "validation_generator = OptimizedDataGenerator(\n",
    "    load_from_tfrecords_dir = tfrecords_dir_val,\n",
    "    shuffle = True,\n",
    "    seed = seed,\n",
    "    quantize = True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c72cef1b-61db-489f-83bb-039a46861094",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 02:49:37.896990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3234 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB MIG 1g.5gb, pci bus id: 0000:01:00.0, compute capability: 8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 13, 21, 20)]      0         \n",
      "                                                                 \n",
      " q_separable_conv2d (QSepar  (None, 11, 19, 5)         285       \n",
      " ableConv2D)                                                     \n",
      "                                                                 \n",
      " q_activation (QActivation)  (None, 11, 19, 5)         0         \n",
      "                                                                 \n",
      " q_conv2d (QConv2D)          (None, 11, 19, 5)         30        \n",
      "                                                                 \n",
      " q_activation_1 (QActivatio  (None, 11, 19, 5)         0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 3, 6, 5)           0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " q_activation_2 (QActivatio  (None, 3, 6, 5)           0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 90)                0         \n",
      "                                                                 \n",
      " q_dense (QDense)            (None, 16)                1456      \n",
      "                                                                 \n",
      " q_activation_3 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_1 (QDense)          (None, 16)                272       \n",
      "                                                                 \n",
      " q_activation_4 (QActivatio  (None, 16)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      " q_dense_2 (QDense)          (None, 14)                238       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2281 (8.91 KB)\n",
      "Trainable params: 2281 (8.91 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=CreateModel((13,21,20),n_filters=5,pool_size=3)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "641b38fa-0cfb-4e17-859f-6570c889d7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fingerprint: 34c2da80\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "pitch = '50x12P5'\n",
    "fingerprint = '%08x' % random.randrange(16**8)\n",
    "base_dir = '/home/dajiang/smart-pixels-ml/weights/weights_7pitches/dataset_3sr_contained_weights/'\n",
    "weights_dir = base_dir + 'weights-{}-bs{}-{}-20t-checkpoints'.format(pitch, batch_size, fingerprint)\n",
    "\n",
    "# create output directories\n",
    "if os.path.isdir(base_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "else:\n",
    "    os.mkdir(base_dir)\n",
    "    os.mkdir(weights_dir)\n",
    "    \n",
    "checkpoint_filepath = weights_dir + '/weights.{epoch:02d}-t{loss:.2f}-v{val_loss:.2f}.hdf5'\n",
    "mcp = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_loss',\n",
    "    save_best_only=False,\n",
    ")\n",
    "\n",
    "print('Model fingerprint: {}'.format(fingerprint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3ecb0d-5db2-4610-bd58-e55d06f049a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 02:49:41.316301: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8906\n",
      "2025-05-13 02:49:41.435123: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-05-13 02:49:41.707539: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2025-05-13 02:49:41.714337: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x76b8600\n",
      "2025-05-13 02:49:41.736834: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fd2a9e78a80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-05-13 02:49:41.736880: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB MIG 1g.5gb, Compute Capability 8.0\n",
      "2025-05-13 02:49:41.741682: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-05-13 02:49:41.798149: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2025-05-13 02:49:41.848906: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 50s 501ms/step - loss: 45374.3984 - val_loss: 16655.3438\n",
      "Epoch 2/1000\n",
      "93/93 [==============================] - 18s 190ms/step - loss: 14994.3350 - val_loss: 13628.5146\n",
      "Epoch 3/1000\n",
      "93/93 [==============================] - 18s 193ms/step - loss: 13014.4141 - val_loss: 12182.2773\n",
      "Epoch 4/1000\n",
      "93/93 [==============================] - 18s 194ms/step - loss: 11757.8311 - val_loss: 11092.9785\n",
      "Epoch 5/1000\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 10758.3623 - val_loss: 10128.8066\n",
      "Epoch 6/1000\n",
      "93/93 [==============================] - 18s 193ms/step - loss: 9718.2861 - val_loss: 9201.3984\n",
      "Epoch 7/1000\n",
      "93/93 [==============================] - 18s 188ms/step - loss: 9117.0586 - val_loss: 8864.3027\n",
      "Epoch 8/1000\n",
      "93/93 [==============================] - 18s 189ms/step - loss: 8899.9287 - val_loss: 8833.4541\n",
      "Epoch 9/1000\n",
      "93/93 [==============================] - 18s 191ms/step - loss: 8748.3584 - val_loss: 8665.1797\n",
      "Epoch 10/1000\n",
      "93/93 [==============================] - 18s 191ms/step - loss: 8679.9873 - val_loss: 8641.3320\n",
      "Epoch 11/1000\n",
      "93/93 [==============================] - 18s 188ms/step - loss: 8650.5459 - val_loss: 8505.4316\n",
      "Epoch 12/1000\n",
      "93/93 [==============================] - 18s 189ms/step - loss: 8596.8066 - val_loss: 8529.9268\n",
      "Epoch 13/1000\n",
      "93/93 [==============================] - 18s 192ms/step - loss: 8556.0254 - val_loss: 8460.0332\n",
      "Epoch 14/1000\n",
      "93/93 [==============================] - 18s 188ms/step - loss: 8516.3076 - val_loss: 8492.9814\n",
      "Epoch 15/1000\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 8444.9912 - val_loss: 8346.9863\n",
      "Epoch 16/1000\n",
      "93/93 [==============================] - 18s 189ms/step - loss: 8370.0400 - val_loss: 8309.5010\n",
      "Epoch 17/1000\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 8360.8242 - val_loss: 8265.7725\n",
      "Epoch 18/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 8329.8359 - val_loss: 8223.9473\n",
      "Epoch 19/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 8284.1338 - val_loss: 8186.7842\n",
      "Epoch 20/1000\n",
      "93/93 [==============================] - 18s 192ms/step - loss: 8253.8760 - val_loss: 8174.9927\n",
      "Epoch 21/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 8272.5029 - val_loss: 8214.9658\n",
      "Epoch 22/1000\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 8309.3789 - val_loss: 8529.1260\n",
      "Epoch 23/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 8262.5625 - val_loss: 8179.5781\n",
      "Epoch 24/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 8233.4219 - val_loss: 8194.3115\n",
      "Epoch 25/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 8227.0020 - val_loss: 8223.7627\n",
      "Epoch 26/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 8177.0967 - val_loss: 8103.3193\n",
      "Epoch 27/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 8148.6899 - val_loss: 8029.2944\n",
      "Epoch 28/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 8124.4854 - val_loss: 8033.9116\n",
      "Epoch 29/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 8127.0845 - val_loss: 8032.3901\n",
      "Epoch 30/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 8106.9077 - val_loss: 8080.2173\n",
      "Epoch 31/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 8094.4609 - val_loss: 8102.2754\n",
      "Epoch 32/1000\n",
      "93/93 [==============================] - 18s 200ms/step - loss: 8075.0254 - val_loss: 8014.3833\n",
      "Epoch 33/1000\n",
      "93/93 [==============================] - 19s 204ms/step - loss: 8012.1099 - val_loss: 7979.2261\n",
      "Epoch 34/1000\n",
      "93/93 [==============================] - 20s 216ms/step - loss: 7987.6504 - val_loss: 7891.5044\n",
      "Epoch 35/1000\n",
      "93/93 [==============================] - 21s 229ms/step - loss: 7968.9268 - val_loss: 7936.9243\n",
      "Epoch 36/1000\n",
      "93/93 [==============================] - 18s 190ms/step - loss: 7983.9165 - val_loss: 7944.8530\n",
      "Epoch 37/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7956.7764 - val_loss: 7890.5464\n",
      "Epoch 38/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7964.1045 - val_loss: 7868.1250\n",
      "Epoch 39/1000\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 7927.1099 - val_loss: 7852.2354\n",
      "Epoch 40/1000\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 7915.7222 - val_loss: 7940.6504\n",
      "Epoch 41/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7917.8594 - val_loss: 7884.5552\n",
      "Epoch 42/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7917.9751 - val_loss: 7840.2295\n",
      "Epoch 43/1000\n",
      "93/93 [==============================] - 19s 198ms/step - loss: 7884.1821 - val_loss: 7790.7847\n",
      "Epoch 44/1000\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 7829.5913 - val_loss: 7851.7412\n",
      "Epoch 45/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7819.2271 - val_loss: 7805.1963\n",
      "Epoch 46/1000\n",
      "93/93 [==============================] - 18s 199ms/step - loss: 7826.8999 - val_loss: 7808.0220\n",
      "Epoch 47/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7815.9404 - val_loss: 7814.0269\n",
      "Epoch 48/1000\n",
      "93/93 [==============================] - 18s 188ms/step - loss: 7805.8057 - val_loss: 7732.0327\n",
      "Epoch 49/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7790.4380 - val_loss: 7774.2852\n",
      "Epoch 50/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7851.5728 - val_loss: 7784.5356\n",
      "Epoch 51/1000\n",
      "93/93 [==============================] - 18s 191ms/step - loss: 7781.4189 - val_loss: 7710.2192\n",
      "Epoch 52/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7745.2305 - val_loss: 7701.2935\n",
      "Epoch 53/1000\n",
      "93/93 [==============================] - 18s 198ms/step - loss: 7741.6528 - val_loss: 7754.6729\n",
      "Epoch 54/1000\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 7770.0308 - val_loss: 7663.2603\n",
      "Epoch 55/1000\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 7718.8740 - val_loss: 7747.7075\n",
      "Epoch 56/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7692.3115 - val_loss: 7661.4990\n",
      "Epoch 57/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7687.7339 - val_loss: 7627.9663\n",
      "Epoch 58/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7692.2588 - val_loss: 7582.5586\n",
      "Epoch 59/1000\n",
      "93/93 [==============================] - 20s 215ms/step - loss: 7673.1509 - val_loss: 7673.8369\n",
      "Epoch 60/1000\n",
      "93/93 [==============================] - 19s 210ms/step - loss: 7694.2637 - val_loss: 7615.3384\n",
      "Epoch 61/1000\n",
      "93/93 [==============================] - 18s 193ms/step - loss: 7663.5508 - val_loss: 7598.7373\n",
      "Epoch 62/1000\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 7661.9092 - val_loss: 7673.1328\n",
      "Epoch 63/1000\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 7638.3022 - val_loss: 7557.1777\n",
      "Epoch 64/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7625.8442 - val_loss: 7575.4849\n",
      "Epoch 65/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7699.6851 - val_loss: 7729.8823\n",
      "Epoch 66/1000\n",
      "93/93 [==============================] - 19s 201ms/step - loss: 7672.5342 - val_loss: 7669.2544\n",
      "Epoch 67/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7647.0610 - val_loss: 7532.2720\n",
      "Epoch 68/1000\n",
      "93/93 [==============================] - 18s 189ms/step - loss: 7640.1323 - val_loss: 7688.7437\n",
      "Epoch 69/1000\n",
      "93/93 [==============================] - 21s 221ms/step - loss: 7610.5518 - val_loss: 7548.3765\n",
      "Epoch 70/1000\n",
      "93/93 [==============================] - 20s 219ms/step - loss: 7578.9141 - val_loss: 7509.2070\n",
      "Epoch 71/1000\n",
      "93/93 [==============================] - 20s 219ms/step - loss: 7572.6973 - val_loss: 7562.9722\n",
      "Epoch 72/1000\n",
      "93/93 [==============================] - 19s 204ms/step - loss: 7595.3965 - val_loss: 7510.4004\n",
      "Epoch 73/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7561.6265 - val_loss: 7535.4922\n",
      "Epoch 74/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7570.1406 - val_loss: 7493.9077\n",
      "Epoch 75/1000\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 7539.9878 - val_loss: 7453.2837\n",
      "Epoch 76/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7539.8237 - val_loss: 7561.0586\n",
      "Epoch 77/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7598.8691 - val_loss: 7513.0801\n",
      "Epoch 78/1000\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 7753.0059 - val_loss: 7617.1387\n",
      "Epoch 79/1000\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 7622.5645 - val_loss: 7587.6631\n",
      "Epoch 80/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7565.6094 - val_loss: 7538.9434\n",
      "Epoch 81/1000\n",
      "93/93 [==============================] - 16s 178ms/step - loss: 7583.4229 - val_loss: 7648.9790\n",
      "Epoch 82/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7724.0430 - val_loss: 7681.2207\n",
      "Epoch 83/1000\n",
      "93/93 [==============================] - 16s 173ms/step - loss: 7620.6411 - val_loss: 7428.8628\n",
      "Epoch 84/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7567.6196 - val_loss: 7501.1167\n",
      "Epoch 85/1000\n",
      "93/93 [==============================] - 18s 199ms/step - loss: 7555.2002 - val_loss: 7690.8096\n",
      "Epoch 86/1000\n",
      "93/93 [==============================] - 20s 212ms/step - loss: 7541.5630 - val_loss: 7475.7134\n",
      "Epoch 87/1000\n",
      "93/93 [==============================] - 20s 210ms/step - loss: 7537.6650 - val_loss: 7431.1494\n",
      "Epoch 88/1000\n",
      "93/93 [==============================] - 18s 194ms/step - loss: 7516.3696 - val_loss: 7454.8496\n",
      "Epoch 89/1000\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 7515.5171 - val_loss: 7443.3184\n",
      "Epoch 90/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7488.5693 - val_loss: 7427.9531\n",
      "Epoch 91/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7461.4434 - val_loss: 7517.7012\n",
      "Epoch 92/1000\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 7502.9258 - val_loss: 7573.9316\n",
      "Epoch 93/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7488.0840 - val_loss: 7441.0796\n",
      "Epoch 94/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7501.9653 - val_loss: 7366.7910\n",
      "Epoch 95/1000\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 7467.7646 - val_loss: 7453.0005\n",
      "Epoch 96/1000\n",
      "93/93 [==============================] - 16s 174ms/step - loss: 7488.2617 - val_loss: 7380.6387\n",
      "Epoch 97/1000\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 7465.2358 - val_loss: 7459.1606\n",
      "Epoch 98/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7481.2690 - val_loss: 7420.8945\n",
      "Epoch 99/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7461.2031 - val_loss: 7422.4341\n",
      "Epoch 100/1000\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 7458.2886 - val_loss: 7508.6035\n",
      "Epoch 101/1000\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 7466.0308 - val_loss: 7495.9634\n",
      "Epoch 102/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7435.3457 - val_loss: 7476.2559\n",
      "Epoch 103/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7422.3765 - val_loss: 7363.2856\n",
      "Epoch 104/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7416.0903 - val_loss: 7383.4321\n",
      "Epoch 105/1000\n",
      "93/93 [==============================] - 16s 171ms/step - loss: 7466.1211 - val_loss: 7427.7397\n",
      "Epoch 106/1000\n",
      "93/93 [==============================] - 16s 178ms/step - loss: 7459.7295 - val_loss: 7399.9619\n",
      "Epoch 107/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7448.6367 - val_loss: 7395.7593\n",
      "Epoch 108/1000\n",
      "93/93 [==============================] - 19s 204ms/step - loss: 7420.1147 - val_loss: 7728.3081\n",
      "Epoch 109/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7406.0039 - val_loss: 7313.7085\n",
      "Epoch 110/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7395.6851 - val_loss: 7328.5332\n",
      "Epoch 111/1000\n",
      "93/93 [==============================] - 16s 175ms/step - loss: 7405.1284 - val_loss: 7456.3184\n",
      "Epoch 112/1000\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 7401.8677 - val_loss: 7298.9805\n",
      "Epoch 113/1000\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 7391.6201 - val_loss: 7362.8359\n",
      "Epoch 114/1000\n",
      "93/93 [==============================] - 16s 169ms/step - loss: 7398.5767 - val_loss: 7318.0420\n",
      "Epoch 115/1000\n",
      "93/93 [==============================] - 16s 178ms/step - loss: 7369.4893 - val_loss: 7500.4380\n",
      "Epoch 116/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7373.3906 - val_loss: 7330.8833\n",
      "Epoch 117/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7500.3779 - val_loss: 7344.1323\n",
      "Epoch 118/1000\n",
      "93/93 [==============================] - 16s 172ms/step - loss: 7436.9648 - val_loss: 7435.5532\n",
      "Epoch 119/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7398.4907 - val_loss: 7452.6348\n",
      "Epoch 120/1000\n",
      "93/93 [==============================] - 16s 176ms/step - loss: 7419.8154 - val_loss: 7411.4395\n",
      "Epoch 121/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7418.8965 - val_loss: 7325.2607\n",
      "Epoch 122/1000\n",
      "93/93 [==============================] - 18s 193ms/step - loss: 7391.3950 - val_loss: 7400.5557\n",
      "Epoch 123/1000\n",
      "93/93 [==============================] - 18s 196ms/step - loss: 7448.9976 - val_loss: 7338.6323\n",
      "Epoch 124/1000\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 7422.6631 - val_loss: 7630.6494\n",
      "Epoch 125/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7413.3882 - val_loss: 7289.1392\n",
      "Epoch 126/1000\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 7384.2251 - val_loss: 7307.2085\n",
      "Epoch 127/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7411.1851 - val_loss: 7279.3589\n",
      "Epoch 128/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7373.6606 - val_loss: 7288.1421\n",
      "Epoch 129/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7357.9521 - val_loss: 7244.9565\n",
      "Epoch 130/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7410.2070 - val_loss: 7384.0410\n",
      "Epoch 131/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7403.0693 - val_loss: 7328.1821\n",
      "Epoch 132/1000\n",
      "93/93 [==============================] - 18s 195ms/step - loss: 7371.5415 - val_loss: 7575.9966\n",
      "Epoch 133/1000\n",
      "93/93 [==============================] - 19s 203ms/step - loss: 7403.0376 - val_loss: 7280.2690\n",
      "Epoch 134/1000\n",
      "93/93 [==============================] - 20s 212ms/step - loss: 7364.7070 - val_loss: 7289.2969\n",
      "Epoch 135/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7385.2197 - val_loss: 7451.0537\n",
      "Epoch 136/1000\n",
      "93/93 [==============================] - 18s 189ms/step - loss: 7375.8760 - val_loss: 7324.4878\n",
      "Epoch 137/1000\n",
      "93/93 [==============================] - 20s 209ms/step - loss: 7347.5063 - val_loss: 7413.2349\n",
      "Epoch 138/1000\n",
      "93/93 [==============================] - 18s 196ms/step - loss: 7325.9277 - val_loss: 7231.7671\n",
      "Epoch 139/1000\n",
      "93/93 [==============================] - 19s 202ms/step - loss: 7341.8330 - val_loss: 7256.3125\n",
      "Epoch 140/1000\n",
      "93/93 [==============================] - 19s 205ms/step - loss: 7332.3398 - val_loss: 7262.3291\n",
      "Epoch 141/1000\n",
      "93/93 [==============================] - 19s 210ms/step - loss: 7339.5469 - val_loss: 7343.4170\n",
      "Epoch 142/1000\n",
      "93/93 [==============================] - 18s 194ms/step - loss: 7330.3413 - val_loss: 7367.8867\n",
      "Epoch 143/1000\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 7390.9751 - val_loss: 7253.4028\n",
      "Epoch 144/1000\n",
      "93/93 [==============================] - 16s 178ms/step - loss: 7446.6270 - val_loss: 7292.1318\n",
      "Epoch 145/1000\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 7346.3135 - val_loss: 7440.1118\n",
      "Epoch 146/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7317.6670 - val_loss: 7479.6348\n",
      "Epoch 147/1000\n",
      "93/93 [==============================] - 16s 175ms/step - loss: 7360.6528 - val_loss: 7246.5342\n",
      "Epoch 148/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7347.0454 - val_loss: 7299.4531\n",
      "Epoch 149/1000\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 7296.4180 - val_loss: 7423.9756\n",
      "Epoch 150/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 7325.7021 - val_loss: 7322.5464\n",
      "Epoch 151/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7298.3877 - val_loss: 7253.4082\n",
      "Epoch 152/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7272.3496 - val_loss: 7209.7446\n",
      "Epoch 153/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7251.4131 - val_loss: 7273.5156\n",
      "Epoch 154/1000\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 7255.5469 - val_loss: 7244.3745\n",
      "Epoch 155/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7277.1265 - val_loss: 7402.3594\n",
      "Epoch 156/1000\n",
      "93/93 [==============================] - 16s 175ms/step - loss: 7238.6196 - val_loss: 7169.7061\n",
      "Epoch 157/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7232.0400 - val_loss: 7195.2925\n",
      "Epoch 158/1000\n",
      "93/93 [==============================] - 16s 178ms/step - loss: 7235.0386 - val_loss: 7262.3979\n",
      "Epoch 159/1000\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 7251.3994 - val_loss: 7194.3057\n",
      "Epoch 160/1000\n",
      "93/93 [==============================] - 19s 199ms/step - loss: 7246.5044 - val_loss: 7227.9419\n",
      "Epoch 161/1000\n",
      "93/93 [==============================] - 18s 189ms/step - loss: 7363.7031 - val_loss: 7604.4624\n",
      "Epoch 162/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7330.9019 - val_loss: 7246.0293\n",
      "Epoch 163/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7302.1431 - val_loss: 7250.1948\n",
      "Epoch 164/1000\n",
      "93/93 [==============================] - 18s 199ms/step - loss: 7367.7104 - val_loss: 8352.4834\n",
      "Epoch 165/1000\n",
      "93/93 [==============================] - 19s 210ms/step - loss: 7422.7676 - val_loss: 7392.5986\n",
      "Epoch 166/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7412.9297 - val_loss: 7371.1411\n",
      "Epoch 167/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7353.3701 - val_loss: 7196.7422\n",
      "Epoch 168/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7321.5688 - val_loss: 7230.8486\n",
      "Epoch 169/1000\n",
      "93/93 [==============================] - 16s 174ms/step - loss: 7321.1782 - val_loss: 7283.9385\n",
      "Epoch 170/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7298.1313 - val_loss: 7333.2300\n",
      "Epoch 171/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7307.3315 - val_loss: 7175.1743\n",
      "Epoch 172/1000\n",
      "93/93 [==============================] - 18s 188ms/step - loss: 7285.9048 - val_loss: 7257.3906\n",
      "Epoch 173/1000\n",
      "93/93 [==============================] - 20s 212ms/step - loss: 7304.2227 - val_loss: 7436.4907\n",
      "Epoch 174/1000\n",
      "93/93 [==============================] - 20s 216ms/step - loss: 7286.1475 - val_loss: 7200.5747\n",
      "Epoch 175/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7263.6377 - val_loss: 7319.3867\n",
      "Epoch 176/1000\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 7283.7207 - val_loss: 7226.7173\n",
      "Epoch 177/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 7259.2832 - val_loss: 7209.8379\n",
      "Epoch 178/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7256.0356 - val_loss: 7235.1187\n",
      "Epoch 179/1000\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 7275.9883 - val_loss: 7313.4150\n",
      "Epoch 180/1000\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 7283.3818 - val_loss: 7564.7368\n",
      "Epoch 181/1000\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 7331.9253 - val_loss: 7264.2544\n",
      "Epoch 182/1000\n",
      "93/93 [==============================] - 18s 196ms/step - loss: 7281.2803 - val_loss: 7359.8638\n",
      "Epoch 183/1000\n",
      "93/93 [==============================] - 20s 211ms/step - loss: 7260.7803 - val_loss: 7184.0674\n",
      "Epoch 184/1000\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 7244.2402 - val_loss: 7185.0356\n",
      "Epoch 185/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7241.7231 - val_loss: 7231.1450\n",
      "Epoch 186/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7238.5112 - val_loss: 7245.8164\n",
      "Epoch 187/1000\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 7198.5400 - val_loss: 7246.4893\n",
      "Epoch 188/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7197.3213 - val_loss: 7170.5083\n",
      "Epoch 189/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7165.4546 - val_loss: 7284.1733\n",
      "Epoch 190/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7193.6602 - val_loss: 7247.2227\n",
      "Epoch 191/1000\n",
      "93/93 [==============================] - 17s 188ms/step - loss: 7240.8594 - val_loss: 7268.2168\n",
      "Epoch 192/1000\n",
      "93/93 [==============================] - 18s 190ms/step - loss: 7205.0767 - val_loss: 7131.6226\n",
      "Epoch 193/1000\n",
      "93/93 [==============================] - 18s 192ms/step - loss: 7173.2988 - val_loss: 7151.1479\n",
      "Epoch 194/1000\n",
      "93/93 [==============================] - 18s 189ms/step - loss: 7177.3384 - val_loss: 7174.2637\n",
      "Epoch 195/1000\n",
      "93/93 [==============================] - 18s 190ms/step - loss: 7189.4438 - val_loss: 7253.3101\n",
      "Epoch 196/1000\n",
      "93/93 [==============================] - 17s 187ms/step - loss: 7163.8262 - val_loss: 7191.5093\n",
      "Epoch 197/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 7187.4150 - val_loss: 7160.5283\n",
      "Epoch 198/1000\n",
      "93/93 [==============================] - 19s 200ms/step - loss: 7212.6963 - val_loss: 7163.7549\n",
      "Epoch 199/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 7197.2007 - val_loss: 7078.0098\n",
      "Epoch 200/1000\n",
      "93/93 [==============================] - 18s 195ms/step - loss: 7221.2144 - val_loss: 7326.8975\n",
      "Epoch 201/1000\n",
      "93/93 [==============================] - 20s 218ms/step - loss: 7183.8159 - val_loss: 7132.1948\n",
      "Epoch 202/1000\n",
      "93/93 [==============================] - 21s 223ms/step - loss: 7209.9912 - val_loss: 7220.5361\n",
      "Epoch 203/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7173.3765 - val_loss: 7071.3340\n",
      "Epoch 204/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7201.1963 - val_loss: 7144.7739\n",
      "Epoch 205/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7196.4062 - val_loss: 7175.7637\n",
      "Epoch 206/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7194.5386 - val_loss: 7063.7866\n",
      "Epoch 207/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7155.9263 - val_loss: 7288.7495\n",
      "Epoch 208/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7160.2856 - val_loss: 7200.5806\n",
      "Epoch 209/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7138.0137 - val_loss: 7085.2515\n",
      "Epoch 210/1000\n",
      "93/93 [==============================] - 17s 177ms/step - loss: 7144.7798 - val_loss: 7151.5610\n",
      "Epoch 211/1000\n",
      "93/93 [==============================] - 16s 175ms/step - loss: 7130.7720 - val_loss: 7084.1504\n",
      "Epoch 212/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7110.1768 - val_loss: 7267.9155\n",
      "Epoch 213/1000\n",
      "93/93 [==============================] - 16s 175ms/step - loss: 7161.8784 - val_loss: 7070.2471\n",
      "Epoch 214/1000\n",
      "93/93 [==============================] - 17s 181ms/step - loss: 7136.6709 - val_loss: 7086.8242\n",
      "Epoch 215/1000\n",
      "93/93 [==============================] - 17s 178ms/step - loss: 7125.9482 - val_loss: 7222.0542\n",
      "Epoch 216/1000\n",
      "93/93 [==============================] - 19s 202ms/step - loss: 7123.9961 - val_loss: 7114.2417\n",
      "Epoch 217/1000\n",
      "93/93 [==============================] - 21s 222ms/step - loss: 7131.3979 - val_loss: 7350.1079\n",
      "Epoch 218/1000\n",
      "93/93 [==============================] - 20s 217ms/step - loss: 7106.9297 - val_loss: 7117.0249\n",
      "Epoch 219/1000\n",
      "93/93 [==============================] - 18s 190ms/step - loss: 7126.7358 - val_loss: 7051.9253\n",
      "Epoch 220/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7109.2715 - val_loss: 7078.3174\n",
      "Epoch 221/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7160.0332 - val_loss: 7323.4116\n",
      "Epoch 222/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 7160.6191 - val_loss: 7336.8501\n",
      "Epoch 223/1000\n",
      "93/93 [==============================] - 18s 191ms/step - loss: 7112.1406 - val_loss: 7170.8975\n",
      "Epoch 224/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7122.1636 - val_loss: 7072.0312\n",
      "Epoch 225/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 7145.0483 - val_loss: 7019.5273\n",
      "Epoch 226/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 7110.7158 - val_loss: 7096.9404\n",
      "Epoch 227/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7089.5884 - val_loss: 7328.3364\n",
      "Epoch 228/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7116.4619 - val_loss: 7162.9810\n",
      "Epoch 229/1000\n",
      "93/93 [==============================] - 20s 214ms/step - loss: 7101.6279 - val_loss: 7031.6699\n",
      "Epoch 230/1000\n",
      "93/93 [==============================] - 21s 222ms/step - loss: 7092.8916 - val_loss: 7021.2153\n",
      "Epoch 231/1000\n",
      "93/93 [==============================] - 19s 206ms/step - loss: 7080.9248 - val_loss: 7025.3228\n",
      "Epoch 232/1000\n",
      "93/93 [==============================] - 20s 216ms/step - loss: 7076.5493 - val_loss: 7021.6616\n",
      "Epoch 233/1000\n",
      "93/93 [==============================] - 20s 217ms/step - loss: 7062.2998 - val_loss: 7093.7544\n",
      "Epoch 234/1000\n",
      "93/93 [==============================] - 21s 221ms/step - loss: 7085.3013 - val_loss: 7081.3633\n",
      "Epoch 235/1000\n",
      "93/93 [==============================] - 20s 220ms/step - loss: 7066.3696 - val_loss: 7011.3120\n",
      "Epoch 236/1000\n",
      "93/93 [==============================] - 18s 195ms/step - loss: 7078.4653 - val_loss: 7020.4971\n",
      "Epoch 237/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7081.0132 - val_loss: 7099.3408\n",
      "Epoch 238/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7061.6953 - val_loss: 7072.0928\n",
      "Epoch 239/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7098.2695 - val_loss: 7055.2139\n",
      "Epoch 240/1000\n",
      "93/93 [==============================] - 17s 179ms/step - loss: 7087.9229 - val_loss: 7239.3765\n",
      "Epoch 241/1000\n",
      "93/93 [==============================] - 18s 196ms/step - loss: 7087.9937 - val_loss: 6993.0991\n",
      "Epoch 242/1000\n",
      "93/93 [==============================] - 20s 218ms/step - loss: 7065.2388 - val_loss: 7066.8730\n",
      "Epoch 243/1000\n",
      "93/93 [==============================] - 20s 218ms/step - loss: 7094.1592 - val_loss: 7024.8066\n",
      "Epoch 244/1000\n",
      "93/93 [==============================] - 19s 197ms/step - loss: 7120.1201 - val_loss: 7007.0337\n",
      "Epoch 245/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7069.9741 - val_loss: 6995.9292\n",
      "Epoch 246/1000\n",
      "93/93 [==============================] - 17s 185ms/step - loss: 7109.0703 - val_loss: 7034.1377\n",
      "Epoch 247/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7053.8745 - val_loss: 7088.9834\n",
      "Epoch 248/1000\n",
      "93/93 [==============================] - 17s 183ms/step - loss: 7060.3931 - val_loss: 7024.0093\n",
      "Epoch 249/1000\n",
      "93/93 [==============================] - 17s 186ms/step - loss: 7103.7324 - val_loss: 7051.0977\n",
      "Epoch 250/1000\n",
      "93/93 [==============================] - 17s 184ms/step - loss: 7120.8374 - val_loss: 7015.7344\n",
      "Epoch 251/1000\n",
      "93/93 [==============================] - 17s 182ms/step - loss: 7044.4673 - val_loss: 6980.0527\n",
      "Epoch 252/1000\n",
      "93/93 [==============================] - 16s 175ms/step - loss: 7077.7935 - val_loss: 7115.5278\n",
      "Epoch 253/1000\n",
      "93/93 [==============================] - 16s 173ms/step - loss: 7029.8481 - val_loss: 6984.7480\n",
      "Epoch 254/1000\n",
      "93/93 [==============================] - 18s 197ms/step - loss: 7021.6143 - val_loss: 6985.6299\n",
      "Epoch 255/1000\n",
      "93/93 [==============================] - 17s 180ms/step - loss: 7036.8896 - val_loss: 7272.0005\n",
      "Epoch 256/1000\n",
      "93/93 [==============================] - 16s 177ms/step - loss: 7036.8652 - val_loss: 6985.7690\n",
      "Epoch 257/1000\n",
      "93/93 [==============================] - ETA: 0s - loss: 7017.7603"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    callbacks=[mcp],\n",
    "                    epochs=1000,\n",
    "                    shuffle=False, # shuffling now occurs within the data-loader\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06b616-1cfc-437c-9a18-836bcb218b31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
